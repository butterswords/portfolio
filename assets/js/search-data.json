{
  
    
        "post0": {
            "title": "Understanding the history of our brewing collective",
            "content": "This is a test post to see if I can make embedding vizzes work on this site. . What’s happening here? . . Trying a different way .",
            "url": "https://butterswords.github.io/portfolio/test",
            "relUrl": "/test",
            "date": " • Feb 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Picking a robot to teach my son to code",
            "content": "From the moment I knew we were going to have a kid, before we knew the gender, I’ve been thinking of ways to get them to engage with robots. I love robots. I want to share that love with those around me. This is not a new thing for me. Years ago I bought a Wonder Workshop Dash robot for my nephew. He didn’t get into it. It was likely not something him and his father wanted to invest the time in to figure out. I realize I gave them a gift that I would want more than something I knew they’d enjoy. Still, it looked like an amazing idea and one I have been carrying around as an idea for my own kids. I have watched that toy for years, looking to see if they made a version compatible with a younger age group. They have not. So, when my son was born I started looking for newer robots and wondered if I could justify getting one for him. . Watching my son explore Duplo on the floor at 2 years old made it clear how much he enjoys putting things together. I spent hours with him, building things and seeing what he’d do with them. He would puzzle over things I’d built. Take them apart. Try to put them back together. Get frustrated. After a while he would put something completely different back together. It surprised me that at 2 and a half he understood the anatomy of planes and automobiles. He’d show me a “plane” he’d built. He’d use two long pieces, often at odd angles for wings. He even included one at the back for a tail. I couldn’t believe he had put them together on his own. It made me proud. It made me want to push him. It made me wonder at what age I could give him a robot. . This post is a place for me to share my thoughts on it for anyone it may help. It is not a review of different robots. I am not an early childhood expert, so take my words as those of a father who spent dozens of hours looking at the options. . Should you teach your kids to code? . Short answer: I think you should expose them to it. I don’t think you should force it on them. . The reasons I have for doing it . The ability to code is in high demand in today’s society. There is a shortage of programmers. The situation will only going to get worse as the need continues to outpace the system’s ability to provide adequate guidance. As technology develops at a rapid pace, I foresee two types of people emerging. There will be the general populace. They will be users of systems, who do not ask how the systems work. Then there will be those who understand how to interrogate systems. The power will likely rest with the latter, so coding will only grow in importance in future generations. Kids who grow up knowing how to code will always have an employable skill. . If done right, coding can teach fundamental skills for the developing mind. Some of the ones I’ve seen: . cause and effect | problem solving | creativity | emotional control | math is fun | . It’s also a great way to bond with my son. Since he’s only 3 I do a lot of the work, and I get to ask him what he thinks we should do or where we should put the square. It’s an interactive process that reminds me how awesome it can be to learn new things. I love seeing him shout “victory”, with an unrepentant glee, when the robot makes it to the end of a run and achieves its goal. I love when he tells me he’s going to make it stop, or when he puts another square down and figures out some novel interaction. . Things that gave me pause . The thing I’m most worried about as a parent is projecting my interests on to my child. I believe programming is critical to the future, but that’s one perspective among many. I can’t pretend to know what the future will hold and I don’t want to try to make my son into a reflection of myself. It took me a while to accept that there’s a difference between exposing him to my interests and forcing them upon him. I will likely struggle with this throughout his life, so it’s good to learn early on where I stand with it. . Another significant concern I have is about expectations. The world imposes a lot of expectations on children and I feel social media is increasing those at an alarming rate. I don’t want to increase pressure on my son by making him think he has to perform at a certain level. I will do what I can to protect him from performance anxiety by making sure we focus on having fun more than anything else. He will learn no matter what, so it’s more critical that he enjoys his robot and decides how far he’s interested in going. . What makes a good system for teaching young kids (2-5) to code? . There are a few key things I was looking for when I researched different options for a robot to teach my son to code. Most of the things I’ve seen are for 5-15 years old and include direct interaction with coding elements, whether limited by software or hardware. I wanted to find something that felt like it would not be too far off where he’s at developmentally to be useful. I finally decided on the four things below: . It has to be screen-free as I don’t want him to constantly be on a phone or tablet | All major elements have to be accessible to him without my intervention | I wanted one that required no ability to read to get the full benefits of it | I hate buying something and then discarding it because it’s been outgrown | . I saw several good options, and learned about one or two more after I’d made my decision. For the moment I’ll refrain from saying what I went with until I am able to talk about the options I found. . What are the options out there? . There are tons of robots out there. Some are made by Fischer Price, some are made by tiny boutiques. The ones I list below are ones I found had a clear educational framework and were as much about the learning as the playing. They all also seemed to be geared towards schools and classrooms, which meant they likely had support for activities or a curriculum. As this is not a “review” I’m going to provide my description and a single pro and con for each. . Sphero Indi . . Sphero has many robots targeted at different age groups. The Indi seemed most geared towards the 3-5 age range. It is a little blue car with color sensors so that when it runs over a mat with a particular color (ex. red) it will take a particular action (ex. stop). It has an app associated with it that kids can use to drive it around, or change how the robot interacts with the colors. . Pro - the app allows it to evolve with the child and provide countless opportunities for creating novel interactions | Con - it only has one type of sensor so it only works with the cards provided (or a very specific color palette that’s hard to find) | . Cubetto . . A cute wooden robot that has a lot of character. It has a separate board to program the motions so that a child can learn they have the ability to affect things from a distance. It has mats that can be used to tell stories and provide more ways for kids to imagine how to interact. . Pro - completely screenless, even the more advanced code elements are done through different shaped pieces | Con - it did not seem capable of evolving with my son so much as they want to buy story packs as a way to increase replayability and creativity | . Cubelets . . Little blocks that attach to each other to create countless interactions. From the website: . Each Cubelet has a special function and belongs to one of three categories: SENSE Cubelets take in information from the robot’s environment. THINK Cubelets compute and change that information, and ACT Cubelets turn that information into physical action. . Pro - Extreme customizability and can interface with lego | Con - The supply chain made it impossible to buy a set | . Botley . . I heard about this one from a friend and former colleague. He got it for his daughter who’s a bit over a year older than my son. It seems like a great educational toy. It’s aimed at 5-9 year olds. It’s for kids a bit older than the others I was looking at and it seems to have a full remote for direct control of the robot. . Pro - it has arms and can be used with specific activities that the other robots do not support due to their design | Con - I don’t know enough about it to give it a con, so I guess that is the con | . Kibo . . This is the first robot I found after my son was born. I’ve been following them for years and it seems like they have an active community in the education space. It’s a completely screenless wooden robot with a lot of customizability. . Pro - you can add a lot of parts to it to make it do different things | Con - the different packs get pricey so it does not feel right for a personal purchase | . What I chose and why . I made my choice after much hemming and hawing. I kept going back and forth on price point, design, and each robots ability to scale with my son’s development. In the end, I decided to go with the Sphero Indi. . It has a solid design, small and lightweight while still being very sturdy. He can hold it in his hands with ease and move it from tile to tile. He has already launched it off the couch once or twice and it seems to take impact from 2-3 feet pretty well. I worried about him running it into things, because it makes an awful noise, but that has not yet caused any issues with its performance. . It has customizability through stickers and changing the color of the lights through the app. This has been a hoot for my son as he loves stickers. . The floor tiles are large and bright, making it easy for my son to interact with and differentiate between them. He loves saying, “Green means go, Red means stop, Yellow means caution!” It’s awesome to watch him explore them. I enjoy it even when he’s refusing to use them as they’re intended because I know he’s figuring his own way through it all. . It has challenge cards that, without any words, teach the mechanics of the robot. There’s also a free pdf that walks through the basics. It’s amazingly intuitive and I can’t wait to just let my kid see it and interact with it. I actually think it’s a perfect case study for teaching design principles to technical writers (but that’s a different post entirely). . Note: As a father I encourage anyone reading this to be more patient with their kids when they lose focus. It is critical to give them space to process when they stop trying to make things work. I admit it frustrates me, especially when it’s accompanied with faux plaintive screaming to try to manipulate me. I know it’s part of my son’s development and I have to control my reactions to ensure he has a great experience. . The cost was reasonable. Compared to some of the others I mention it ranges from $80-120, depending on if you get the school version or the home version. I went with the school version because I want the higher quality materials that will last longer. Even spending the extra money it’s a hundred dollars less than some of the other options. . Lastly, the app provides enough additional functionality that I know my son will be able to use this for years to come without outgrowing it. By that time I hope he’ll be ready to teach a younger sibling how to use it. . I’m excited to watch my son learn how to use his robot. I can’t wait to see him figure out some of the challenges and start using the app to customize how the robot works. I know that it’ll take years for him to master the robot and I am going to enjoy every moment. .",
            "url": "https://butterswords.github.io/portfolio/journey/robots/fatherhood/2022/01/23/picking-a-robot-to-teach-code.html",
            "relUrl": "/journey/robots/fatherhood/2022/01/23/picking-a-robot-to-teach-code.html",
            "date": " • Jan 23, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Reflections on my first Kaggle Collaboration",
            "content": "Transitioning from coursework to practical application is difficult. It’s one thing to take courses on Data Analysis for Social Science or Probability - The Science of Uncertainty and Data. They provide a solid foundation and theoretical framework to work with. It’s another to apply that and achieve results. I struggled a lot with finding ways to make use of what I’m learning because my previous jobs have been in SEO (Search Engine Optimization) and did not afford me the time or freedom to do it through my work. In November of 2021 I reached out to a friend of mine to see if he had any interest in exploring machine learning. To my great joy it turned out he did and we agreed to team up and try to do some Kaggle competitions together. . We decided to tackle the Natural Language Processing with Disaster Tweets as a good entry point into the world of Kaggle Competitions. The rest of this post describes our process and my personal learnings. . Note: I am focusing on NLP (Natural Language Processing) because there are many ways I can incorporate it into my work, which will only serve to reinforce everything I’m learning along the way. . Setting goals . I find a good place to start any project is with concrete goals and how to work together. It comes from years managing projects across organizations and clients where success depends on how well you understand the end state at the front. We took time in our first meeting to set the groundwork, below are the list of the goals we came up with. . The goals of our first project: . Test Kaggle Notebooks to see how they work and the odd nuances that come with a specific product | Classify tweets as 1 (pertaining to a disaster) or 0 (not pertaining to a disaster) | Get comfortable working as a team | Get a better sense of using NLP for text classification | . Developing a process that works for collaborating . While we had explicit goals, we did not have any process for achieving them. We’re two friends after all and so we just decided to wing it. That worked surprisingly well, all things considered. I did a lot of research and exploration and then we worked together to refine things as we ran into problems. . Merging teams and then code . Kaggle has some really excellent tools that you get access to for free. Each person having the ability to work in a notebook with access to GPUs is excellent. We initially started our work separately, and then about two weeks into the process we decided to merge our work and create a team. The creating a team part was simple and seamless. The merging of code was completely manual. I can’t blame Kaggle for that, they give you a bunch of stuff for free. It’s just one of those aspects of the learning curve that I feel is unavoidable. If I were to do it again I would say: . Do not start your project in separate notebooks on Kaggle unless you plan to merge very early | Make sure the person who has more detailed code becomes team lead | Copy your notebook locally before merging teams so that you can merge code later with ease | . Once the team was formed we had to figure out how we were going to do all of our future development. This lead to some interesting discoveries about versioning on Kaggle. . Developing in Github vs Kaggle . Once we had built a team we both tried to go into the notebook and start to work synchronously. First, there’s no functionality to allow for simultaneous editing (not a big deal as it’s all free) and no concept of branches or pull requests that I could find. This became much more problematic as we realized that Kaggle doesn’t always default you to the last edited version, but rather the last version you touched. When you want to go to an updated version you have to “revert” to it, which doesn’t really make sense if you’re on v8 and the latest is v14. This through me for a loop more than once. . A saving grace, at least for me, was that Kaggle allows you to import notebooks directly from Github. That allowed me to download the latest version and then work locally (like a branch native to Github) and then I would pull from Github when I was ready to push it back into the Kaggle environment. The functionality for that is very basic, however, which made for some interesting renaming of repositories. . . There’s a quickselect for Github, which is nice. | The search functionality is powerful, but flawed. There should be an option to select only from private repositories connected to your account, not just an inclusion of those in your search results. | The upload is nearly instantaneous depending on size of the file and your internet speed | It was much easier for me to develop in Github than directly in Kaggle because I could use my Jupyter Lab settings and keep files locally. One caveat, you have to maintain a line of code for loading data in each cell that allows you to switch between environments. I would likely consider adding a helper class for much larger projects that detects the environment and automatically updates the path for reading files. Though manually switching, see below, isn’t too problematic if you have limited inputs. . &#39;&#39;&#39;In this scenario I am editing my notebook on my local machine. So I have commented out the kaggle-specific pathing.&#39;&#39;&#39; #sample = pd.read_csv(&quot;/kaggle/input/nlp-getting-started/sample_submission.csv&quot;) sample = pd.read_csv(&quot;sample_submission.csv&quot;) # For Local machine . At the end of the day, I think it makes a lot more sense to develop non-GPU code in Github. You can then import the notebook into Kaggle to train the models. . note: I have not tried this for GPU-specific projects, like deeplearning for NLP or Computer Vision yet. I may update this line based on my experience when I get there. . Developing different methodologies based on different tutorials . Developing asynchronously, we each took a different way to exploring approaches to the problem. We found different Kaggle Tutorials and that led to some very interesting early discoveries. We realized that there were so many ways to approach the problem that it made sense to test our work against each other and see if and how we could improve on one another’s work. . My friend used an NLP Getting Started Tutorial that is widely viewed and used (roughly 100K views and 2.3K copies). It achieves a base accuracy of around 78%. . I took the approach of incorporating some of the work I was doing through codecademy’s Data Science Career Path with SKLearn’s tutorial on NLP: Working With Text Data. This lead me down several deep rabbit holes, but was richly rewarding for a better understanding of creating pipelines with sklearn, more on that later. My final model achieved an accuracy of 82.08%, which is not an insignificant improvement over the original. Though admittedly it was more complicated and less efficient. . Below is a simple look at all the libraries we ended up including based on our work together. . #Import all major libraries we&#39;ll need to create and run our models import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split #Standard for building Training and Test sets in data from sklearn.preprocessing import StandardScaler # To process what we need from sklearn.model_selection import GridSearchCV #To optimize our model&#39;s parameters from sklearn.pipeline import make_pipeline #To make a pipeline for GridSearchCV from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer #Two of our text processing pipelines from sklearn.svm import LinearSVC, SVC #Loading a Support Vector algorithm to classify the text from sklearn.linear_model import RidgeCV, LogisticRegression #for StackingRegressor from sklearn.naive_bayes import MultinomialNB #Loading a Bayesian algorithm to see if it&#39;s more accurate from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay #For Identifying the type of errors from sklearn.ensemble import StackingClassifier, RandomForestClassifier #for attempting to combine multiple models . Testing methodologies, comparing results, and choosing a model to submit . Because we took different approaches we wanted to formalize how we evaluated and compared our models to see what we were going to submit. This was mainly for ease of our communication as Kaggle allowed us to submit 5 times a day, and we could do it for as long as we like. We ended up using simple line of code recommended in the documentation and almost every tutorial to evaluate accuracy. . print(f&quot;Accuracy: {svc2.score(X_test, y_test) * 100:.3f}%&quot;) . It worked well to gauge between models on the test data. Where it was really informative was in showing us how the performance on test data compared to the competition data. I want to take a moment to note that I use test here because it is the language used in sklearn.model_selection.train_test_split and it is better to consider our test data the validation data based on my understanding from the FastAI course. . Key learnings from the experience . I learned a lot in tackling that first competition. Some of it was mundane (Ex. how to set up environments and move between them) while others were truly enlightening. The ones I will carry with me and want to share are outlined below. . Collaboration . Working in the same file can be laborious without clear boundaries . As I’ve mentioned previously, Kaggle’s inability to provide branches makes it rather difficult to work on different methods in the same workbook. In addition, having multiple models and preprocessing gets to be a terrifying prospect as your notebook grows and grows alongside your experiments. I ended up commenting out all of my friend’s work in the end so that I could make sure I didn’t accidentally overwrite any of my own or refer to a dependency in the wrong form. . Communicate clearly what libraries you intend to use | Have standardized naming conventions | Make sure you use unique names for models, data sets, training sets, etc. so that you don’t run into broken dependencies | . A system’s ability to do simultaneous version control is an active constraint for group work . I’m not a developer and so I haven’t used Github extensively. I’ve had an account on and off for years, hoping to dive deeper but without collaboration it just didn’t seem to matter. This project opened my eyes to the criticality of version control, branches, and pull requests. I strongly recommend that any collaborative development be done in an environment that allows for those three things (at the least) to ensure you don’t break everything. . Data Science . Experimentation with preprocessing leads to insights about the data . Throughout the work I tried various ways of preprocessing the data to make it fit different models in the documentation. I lokoed across multiple sources and pieced them together to create a few things that were novel to me. Below is one example. I wanted to see if adding in indicator variables for missing data could improve the performance of my models. It didn’t, but it lead to me figuring out how to do some very basic data augmentation, something I know I’ll be exploring more as I continue to learn. . #Here we&#39;ll fill Keywords and Location with &quot;Not Available&quot; to see if this can improve our model. We&#39;ll also add indicator columns for missing data to see if that&#39;s important. def add_indicator(col): def wrapper(df): return df[col].isna().astype(int) return wrapper training_clean = training.assign(kw_miss=add_indicator(&quot;keyword&quot;)) training_clean = training_clean.assign(loc_miss=add_indicator(&quot;location&quot;)) training_clean = training_clean.fillna(&quot;Not Available&quot;) training_clean = training_clean.drop(columns=&quot;target&quot;) . Experimentation from documentation produces limited performance increases . Following some exploration of different architectures within the sklearn documentation I ended up submitting a StackingClassifier to the competition and achieved 82.01% accuracy. This was a marginal improvement over our previous attempts that ranged from 80% to 81.4%. However, the really cool thing I found in doing it was learning more about how to create pipelines that allow for combining different methods of classification to increase performance. I had one pipeline that used a CountVectorizer to preprocess the training data for RandomForestClassifier and SVC classifiers. I also used a TfidfVectorizer that preprocessed the data for MultinomialNB and LogisticGregression classifiers. This allowed me to combine four separate estimators before learning the final classifier on another SVC. While this is an inefficient way to train models, especially as it did not provide significant performance. I found it to be a great opportunity to explore different workflows. . Below is a picture of the Confusion Matrix for the final StackingClassifier that highlights how it performed. Doing this over again I would look deeper into what was causing the false negatives to see if I needed to do data augmentation or select better hyperparameters to improve the model by a larger margin. . . My biggest takeaway regarding Data Science is that experimentation and adjusting hyperparameters are critical parts of developing an accurate model. It feels a bit like trial and error, relying heavily on intuition, and you have to understand how they affect the underlying architecture, individually and in aggregate, to make the most of them. This is part of why I am working my way through the FastAI Course. I want to develop better intuitions and a deeper understanding of the theory underpinning the architectures so I can make better choices. .",
            "url": "https://butterswords.github.io/portfolio/kaggle/nlp/journey/2022/01/17/my-first-real-kaggle-collaboration.html",
            "relUrl": "/kaggle/nlp/journey/2022/01/17/my-first-real-kaggle-collaboration.html",
            "date": " • Jan 17, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello, my name is Nathan Butters. Feel free to call me “Nathan” or “Butters” as you see fit. Historically, ~50% of everyone I meet calls me “Butters” and I am happy to let you pick what works best for you. . For more than ten years, I have provided companies with data-driven strategies for content, user experience, product features, and information architecture based on the needs of their customers. I use visualization to tell the stories within the data and improve how goals are created and then measured. I build more efficient, scalable processes everywhere I am because a solid process can lead to great change. Recently I find myself more focused on the creation of products to empower marketers to self-serve while adhering to centralized best practices and guidance. . This site is a place for me to showcase the work I’m doing in Data Science. The work I do for my day job is protected, so this site will focus first on projects I’m doing on my own time. If I think there are applicable lessons to share from projects I’ve done I will share those without referencing anything protected. . So, who am I? (Literally, imagine I sound like this) . Outside of work I can be found dancing/singing or telling stories to my son, Ezra, who is 3 years old; practicing Data Science, right now I’m working through the FastAI Course; coding robots, gopigo.io; playing board games and collaborative storytelling; golfing, as often as I possibly can; hiking; writing stories; and brewing beer. If you have an interest in any of these things feel free to connect with me and we’ll chat about them. I’m also always looking for more people to talk to about Data Science, or to play golf with. I’ll take what I can get. . The rest of this section just outlines useful information to keep in mind. I don’t know that it can cover everything, and I’ll update it if you tell me there’s a section that’d be really useful. . My personal leadership philosophy . Support one another with courage and compassion; prioritize the shared good to achieve great success. . Quotes that speak to the core of who I am . “Work smarter, not harder; do not balk at hard work.” - Nathan Butters 1 . “True heroism is remarkably sober, very undramatic. It is not the urge to surpass all others at whatever cost, but the urge to serve others at whatever cost.” ~Arthur Ashe 2 . “Moral courage is the most valuable and usually the most absent characteristic in men.” ~George S. Patton 3 . “Words are pale shadows of forgotten names. As names have power, words have power. Words can light fires in the minds of men. Words can wring tears from the hardest hearts.” ~Patrick Rothfuss, Name of the Wind 4 . Some additional context: I am aware people misuse the first half of this phrase all the time. Sometimes you’re working as efficiently as possible. Sometimes you don’t have control over your work and the only way to get the job done is to work harder. That’s why both sides of the quote are so important. &#8617; . | Here’s the Wikipedia Article on Arthur Ashe &#8617; . | Here’s the Wikipedia Article on Patton I recognize the person as being problematic, more as I’ve gotten older, yet the quote remains powerful. &#8617; . | Here’s the Wikipedia Article on the book &#8617; . |",
          "url": "https://butterswords.github.io/portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://butterswords.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}